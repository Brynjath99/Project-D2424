{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final Model Setup + Cosine Annealing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5OGuMEf3YDJ",
        "outputId": "0b861658-5b77-4f5a-97e6-0f8fe8900542"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# baseline model with dropout on the cifar10 dataset\n",
        "import sys\n",
        "import tensorflow\n",
        "from tensorflow import keras\n",
        "from matplotlib import pyplot\n",
        "from keras.datasets import cifar10\n",
        "#from keras.utils import to_categorical\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import AveragePooling2D\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "#from keras.optimizers import SGD\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from keras.regularizers import l2\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.optimizers.schedules import CosineDecayRestarts\n",
        "\n",
        "# load train and test dataset\n",
        "def load_dataset():\n",
        "\t# load dataset\n",
        "\t(trainX, trainY), (testX, testY) = cifar10.load_data()\n",
        "\t# one hot encode target values\n",
        "\ttrainY = to_categorical(trainY)\n",
        "\ttestY = to_categorical(testY)\n",
        "\treturn trainX, trainY, testX, testY\n",
        " \n",
        "# scale pixels\n",
        "def prep_pixels(train, test):\n",
        "\t# convert from integers to floats\n",
        "\ttrain_norm = train.astype('float32')\n",
        "\ttest_norm = test.astype('float32')\n",
        "\t# normalize to range 0-1\n",
        "\ttrain_norm = train_norm / 255.0\n",
        "\ttest_norm = test_norm / 255.0\n",
        "\t# return normalized images\n",
        "\treturn train_norm, test_norm"
      ],
      "metadata": {
        "id": "hoz-2KzG3dKu"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "def Standardization(X1,X2):\n",
        "  scaler = StandardScaler()\n",
        "  \n",
        "  nsamples, nx, ny,nz = X1.shape\n",
        "  d1_train_dataset = X1.reshape((nsamples,nx*ny*nz))\n",
        "  standardized_1 = scaler.fit_transform(d1_train_dataset)\n",
        "\n",
        "  nsampless, nxx, nyy,nzz = X2.shape\n",
        "  d2_train_dataset = X2.reshape((nsampless,nxx*nyy*nzz))\n",
        "  standardized_2 = scaler.fit_transform(d2_train_dataset)\n",
        "  \n",
        "  return standardized_1,standardized_2"
      ],
      "metadata": {
        "id": "d7jfpm5Q3mvv"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "trainX, trainY, testX, testY = load_dataset()\n",
        "# prepare pixel data\n",
        "trainX, testX = prep_pixels(trainX, testX)"
      ],
      "metadata": {
        "id": "trdJFXMH3nI8"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define cnn model\n",
        "def define_model_two():\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))\n",
        "\tmodel.add(BatchNormalization())\n",
        "\tmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "\tmodel.add(BatchNormalization())\n",
        "\tmodel.add(MaxPooling2D((2, 2)))\n",
        "\tmodel.add(Dropout(0.2))\n",
        "\tmodel.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "\tmodel.add(BatchNormalization())\n",
        "\tmodel.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "\tmodel.add(BatchNormalization())\n",
        "\tmodel.add(MaxPooling2D((2, 2)))\n",
        "\tmodel.add(Dropout(0.3))\n",
        "\tmodel.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "\tmodel.add(BatchNormalization())\n",
        "\tmodel.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "\tmodel.add(BatchNormalization())\n",
        "\tmodel.add(MaxPooling2D((2, 2)))\n",
        "\tmodel.add(Dropout(0.4))\n",
        "\tmodel.add(Flatten())\n",
        "\tmodel.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
        "\tmodel.add(BatchNormalization())\n",
        "\tmodel.add(Dropout(0.5))\n",
        "\tmodel.add(Dense(10, activation='softmax'))\n",
        "  # Cosine annealing \n",
        "\tfirst_decay_steps = 1000\n",
        "\tinitial_learning_rate = 0.001\n",
        "\tlr = CosineDecayRestarts(initial_learning_rate, first_decay_steps, t_mul=2.0, m_mul=1.0, alpha=0.0, name=None)\n",
        "\t# compile model\n",
        "\topt = Adam(lr)\n",
        "\t# compile model\n",
        "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\treturn model"
      ],
      "metadata": {
        "id": "xE_wWvVS3piS"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def summarize_diagnostics(history):\n",
        "\tfig, (ax1, ax2) = plt.subplots(2)\n",
        "\t# plot loss\n",
        "\tax1.set_title('Cross Entropy Loss')\n",
        "\tax1.plot(history.history['loss'], color='blue', label='train')\n",
        "\tax1.plot(history.history['val_loss'], color='orange', label='test')\n",
        "\t# plot accuracy\n",
        "\tax2.set_title('Classification Accuracy')\n",
        "\tax2.plot(history.history['accuracy'], color='blue', label='train')\n",
        "\tax2.plot(history.history['val_accuracy'], color='orange', label='test')\n",
        "\tfig.tight_layout()\n",
        "\t# save plot to file\n",
        "\tfilename = 'Enter filename'\n",
        "\tplt.savefig(filename + '_plot.png')\n",
        "\tplt.show()"
      ],
      "metadata": {
        "id": "XZDMDCTI4Gyp"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run the test harness for evaluating a model\n",
        "def run_test_harness():\n",
        "  # load dataset\n",
        "  trainX, trainY, testX, testY = load_dataset()\n",
        "  # prepare pixel data\n",
        "  trainX, testX = prep_pixels(trainX, testX)\n",
        "  trainX, testX =Standardization(trainX, testX)\n",
        "  # Reshape\n",
        "  nsamples, nx =trainX.shape\n",
        "  trainX = trainX.reshape((nsamples,32,32,3))\n",
        "  nnsamples, nnx =testX.shape\n",
        "  testX = testX.reshape((nnsamples,32,32,3))\n",
        "  # define model\n",
        "  model = define_model_two()\n",
        "  history = model.fit(trainX, trainY, epochs=100, batch_size=64, validation_data=(testX, testY), verbose=0)\n",
        "  model.save('final_model_two.h5')\n",
        "  # evaluate model\n",
        "  _, acc = model.evaluate(testX, testY, verbose=0)\n",
        "  print('> %.3f' % (acc * 100.0),\"%\")\n",
        "  # learning curves\n",
        "  summarize_diagnostics(history)\n",
        "\n",
        "# entry point, run the test harness\n",
        "run_test_harness() #30 MINS"
      ],
      "metadata": {
        "id": "4EpuNC9N3uNJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}